{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIL5SYOYmgN6E7VJgvQOjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a63677c481054b548210b6cd931d599e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63e5f25e9400412daca95e419be8ae53",
              "IPY_MODEL_9d7e22bf6b29428c86959bfabb61d2a0",
              "IPY_MODEL_4b2287db314c498a84ecd3f62f6bcaa2"
            ],
            "layout": "IPY_MODEL_5a09397ba91645649bc56a6c04b67077"
          }
        },
        "63e5f25e9400412daca95e419be8ae53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71a47ae8d6f4d77891b8a22c80f3ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_e014de9c1b8d4014819e18cd97f790bd",
            "value": "#0: 100%"
          }
        },
        "9d7e22bf6b29428c86959bfabb61d2a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67d15e8db89047329f6ec3925949d7af",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98abd3a2874749fd9324852c4a73b447",
            "value": 1
          }
        },
        "4b2287db314c498a84ecd3f62f6bcaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d318b3468574316b4ddfbfcbcb6be74",
            "placeholder": "​",
            "style": "IPY_MODEL_a31000fd84eb45e5a5c00dd49ce3e611",
            "value": " 1/1 [00:00&lt;00:00, 10.48ex/s]"
          }
        },
        "5a09397ba91645649bc56a6c04b67077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71a47ae8d6f4d77891b8a22c80f3ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e014de9c1b8d4014819e18cd97f790bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67d15e8db89047329f6ec3925949d7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98abd3a2874749fd9324852c4a73b447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d318b3468574316b4ddfbfcbcb6be74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a31000fd84eb45e5a5c00dd49ce3e611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8564ce260d124dc1b520ccb28a790d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c598d83784e4e818940107d1ab5581a",
              "IPY_MODEL_af1d50b7b1ae4130ab928c64bcb23496",
              "IPY_MODEL_2cb3ddf5fe684a7cbbf88b9c5b4da33d"
            ],
            "layout": "IPY_MODEL_40b1cbeee1ae467d9c1c62ed04622ca0"
          }
        },
        "5c598d83784e4e818940107d1ab5581a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa8093dd637461aa278116d8fc08b54",
            "placeholder": "​",
            "style": "IPY_MODEL_dbe04a66d77d4168b295a20c87081fb5",
            "value": "#1: 100%"
          }
        },
        "af1d50b7b1ae4130ab928c64bcb23496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af6d5ac5316049d2bb3a98d13dfc1b45",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919fd43389eb49a9b30f0036b11ccda9",
            "value": 1
          }
        },
        "2cb3ddf5fe684a7cbbf88b9c5b4da33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896cc517e3e54c35b08f8454f51b0fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_4f26104b947a45a4aa7ff4f55bffcd6a",
            "value": " 1/1 [00:00&lt;00:00, 11.22ex/s]"
          }
        },
        "40b1cbeee1ae467d9c1c62ed04622ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa8093dd637461aa278116d8fc08b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe04a66d77d4168b295a20c87081fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af6d5ac5316049d2bb3a98d13dfc1b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "919fd43389eb49a9b30f0036b11ccda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "896cc517e3e54c35b08f8454f51b0fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f26104b947a45a4aa7ff4f55bffcd6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaronannecchiarico/musiclm-pytorch/blob/main/musiclm_pytorch_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Requirements"
      ],
      "metadata": {
        "id": "06KyYKyj0fdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# If this doesn't work, there's no GPU available or detected"
      ],
      "metadata": {
        "id": "RuvNPhDSzqdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eCvkLB7uN4v",
        "outputId": "29db6d82-5cbc-448b-b112-f4e8da988ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting musiclm-pytorch\n",
            "  Downloading musiclm_pytorch-0.0.12-py3-none-any.whl (10 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vector-quantize-pytorch>=1.0.0\n",
            "  Downloading vector_quantize_pytorch-1.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting einops>=0.6\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting audiolm-pytorch>=0.10.4\n",
            "  Downloading audiolm_pytorch-0.10.4-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (from musiclm-pytorch) (0.13.1+cu116)\n",
            "Collecting x-clip\n",
            "  Downloading x_clip-0.12.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype\n",
            "  Downloading beartype-0.12.0-py3-none-any.whl (754 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m754.5/754.5 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from musiclm-pytorch) (1.13.1+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from audiolm-pytorch>=0.10.4->musiclm-pytorch) (4.64.1)\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ema-pytorch\n",
            "  Downloading ema_pytorch-0.1.4-py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from audiolm-pytorch>=0.10.4->musiclm-pytorch) (1.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from audiolm-pytorch>=0.10.4->musiclm-pytorch) (1.2.0)\n",
            "Collecting Mega-pytorch\n",
            "  Downloading Mega_pytorch-0.0.12-py3-none-any.whl (6.4 kB)\n",
            "Collecting local-attention>=1.5.7\n",
            "  Downloading local_attention-1.5.8-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->musiclm-pytorch) (4.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate->musiclm-pytorch) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate->musiclm-pytorch) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->musiclm-pytorch) (23.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->musiclm-pytorch) (5.4.8)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from x-clip->musiclm-pytorch) (0.14.1+cu116)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from x-clip->musiclm-pytorch) (2022.6.2)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (0.29.33)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.8/264.8 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (1.15.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->x-clip->musiclm-pytorch) (0.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from Mega-pytorch->audiolm-pytorch>=0.10.4->musiclm-pytorch) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->audiolm-pytorch>=0.10.4->musiclm-pytorch) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->x-clip->musiclm-pytorch) (2.25.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->x-clip->musiclm-pytorch) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers->audiolm-pytorch>=0.10.4->musiclm-pytorch) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (5.10.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (4.9.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (0.8.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->x-clip->musiclm-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->x-clip->musiclm-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->x-clip->musiclm-pytorch) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->x-clip->musiclm-pytorch) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq->audiolm-pytorch>=0.10.4->musiclm-pytorch) (3.12.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=ed0712b78f91034fa12a1f767f6546ee6c2e65ec8f6b425b367679f7ace9b489\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: tokenizers, sentencepiece, bitarray, antlr4-python3-runtime, portalocker, omegaconf, ftfy, einops, colorama, beartype, vector-quantize-pytorch, sacrebleu, Mega-pytorch, local-attention, hydra-core, huggingface-hub, ema-pytorch, accelerate, x-clip, transformers, fairseq, audiolm-pytorch, musiclm-pytorch\n",
            "Successfully installed Mega-pytorch-0.0.12 accelerate-0.16.0 antlr4-python3-runtime-4.8 audiolm-pytorch-0.10.4 beartype-0.12.0 bitarray-2.7.0 colorama-0.4.6 einops-0.6.0 ema-pytorch-0.1.4 fairseq-0.12.2 ftfy-6.1.1 huggingface-hub-0.12.0 hydra-core-1.0.7 local-attention-1.5.8 musiclm-pytorch-0.0.12 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.0 vector-quantize-pytorch-1.0.0 x-clip-0.12.0\n"
          ]
        }
      ],
      "source": [
        "pip install musiclm-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install datasets[audio] yt-dlp"
      ],
      "metadata": {
        "id": "CsHn6W4T094K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "qKw_h3ohz0Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer, MuLaNEmbedQuantizer, MusicLM\n",
        "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
        "\n"
      ],
      "metadata": {
        "id": "4NVQRFOWzxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train MuLaN"
      ],
      "metadata": {
        "id": "XOTZ2LWd0X6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transformer = AudioSpectrogramTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 128,\n",
        "    spec_n_fft = 128,\n",
        "    spec_win_length = 24,\n",
        "    spec_aug_stretch_factor = 0.8\n",
        ")\n",
        "\n",
        "text_transformer = TextTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 128\n",
        ")\n",
        "\n",
        "mulan = MuLaN(\n",
        "    audio_transformer = audio_transformer,\n",
        "    text_transformer = text_transformer\n",
        ")\n",
        "\n",
        "# get a ton of <sound, text> pairs and train\n",
        "\n",
        "wavs = torch.randn(2, 512)\n",
        "texts = torch.randint(0, 20000, (2, 256))\n",
        "\n",
        "loss = mulan(wavs, texts)\n",
        "loss.backward()\n",
        "\n",
        "# after much training, you can embed sounds and text into a joint embedding space\n",
        "# for conditioning the audio LM\n",
        "\n",
        "embeds = mulan.get_audio_latents(wavs)  # during training\n",
        "\n",
        "embeds = mulan.get_text_latents(texts)  # during inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4TvhrZCuUTA",
        "outputId": "15472280-1839-42ea-8c9f-dc4a673cdaa7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spectrogram yielded shape of (65, 43), but had to be cropped to (64, 32) to be patchified for transformer\n",
            "spectrogram yielded shape of (65, 43), but had to be cropped to (64, 32) to be patchified for transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
        "\n",
        "quantizer = MuLaNEmbedQuantizer(\n",
        "    mulan = mulan,                          # pass in trained mulan from above\n",
        "    conditioning_dims = (512, 512, 512), # say all three transformers have model dimensions of 1024\n",
        "    namespaces = ('semantic', 'coarse', 'fine')\n",
        ")\n",
        "\n",
        "# now say you want the conditioning embeddings for semantic transformer\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSO7Q4mwu4lt",
        "outputId": "f3f6d92f-8182-4fea-92bc-d1181c928b52"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spectrogram yielded shape of (65, 86), but had to be cropped to (64, 80) to be patchified for transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "4iIM6vrp00GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_clip(\n",
        "    video_identifier,\n",
        "    output_filename,\n",
        "    start_time,\n",
        "    end_time,\n",
        "    tmp_dir='/tmp/musiccaps',\n",
        "    num_attempts=5,\n",
        "    url_base='https://www.youtube.com/watch?v='\n",
        "):\n",
        "    status = False\n",
        "\n",
        "    command = f\"\"\"\n",
        "        yt-dlp --quiet --no-warnings -x --audio-format wav -f bestaudio -o \"{output_filename}\" --download-sections \"*{start_time}-{end_time}\" {url_base}{video_identifier}\n",
        "    \"\"\".strip()\n",
        "\n",
        "    attempts = 0\n",
        "    while True:\n",
        "        try:\n",
        "            output = subprocess.check_output(command, shell=True,\n",
        "                                                stderr=subprocess.STDOUT)\n",
        "        except subprocess.CalledProcessError as err:\n",
        "            attempts += 1\n",
        "            if attempts == num_attempts:\n",
        "                return status, err.output\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Check if the video was successfully saved.\n",
        "    status = os.path.exists(output_filename)\n",
        "    return status, 'Downloaded'\n",
        "\n",
        "\n",
        "def main(\n",
        "    data_dir: str,\n",
        "    sampling_rate: int = 44100,\n",
        "    limit: int = None,\n",
        "    num_proc: int = 1,\n",
        "    writer_batch_size: int = 1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Download the clips within the MusicCaps dataset from YouTube.\n",
        "    Args:\n",
        "        data_dir: Directory to save the clips to.\n",
        "        sampling_rate: Sampling rate of the audio clips.\n",
        "        limit: Limit the number of examples to download.\n",
        "        num_proc: Number of processes to use for downloading.\n",
        "        writer_batch_size: Batch size for writing the dataset. This is per process.\n",
        "    \"\"\"\n",
        "\n",
        "    ds = load_dataset('google/MusicCaps', split='train')\n",
        "    if limit is not None:\n",
        "        print(f\"Limiting to {limit} examples\")\n",
        "        ds = ds.select(range(limit))\n",
        "\n",
        "    data_dir = Path(data_dir)\n",
        "    data_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    def process(example):\n",
        "        outfile_path = str(data_dir / f\"{example['ytid']}.wav\")\n",
        "        status = True\n",
        "        if not os.path.exists(outfile_path):\n",
        "            status = False\n",
        "            status, log = download_clip(\n",
        "                example['ytid'],\n",
        "                outfile_path,\n",
        "                example['start_s'],\n",
        "                example['end_s'],\n",
        "            )\n",
        "\n",
        "        example['audio'] = outfile_path\n",
        "        example['download_status'] = status\n",
        "        return example\n",
        "\n",
        "    return ds.map(\n",
        "        process,\n",
        "        num_proc=num_proc,\n",
        "        writer_batch_size=writer_batch_size,\n",
        "        keep_in_memory=False\n",
        "    ).cast_column('audio', Audio(sampling_rate=sampling_rate))"
      ],
      "metadata": {
        "id": "X_Of87mq0-uZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main('./music_data', num_proc=2, limit=2) # change limit for larger dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "a63677c481054b548210b6cd931d599e",
            "63e5f25e9400412daca95e419be8ae53",
            "9d7e22bf6b29428c86959bfabb61d2a0",
            "4b2287db314c498a84ecd3f62f6bcaa2",
            "5a09397ba91645649bc56a6c04b67077",
            "a71a47ae8d6f4d77891b8a22c80f3ed2",
            "e014de9c1b8d4014819e18cd97f790bd",
            "67d15e8db89047329f6ec3925949d7af",
            "98abd3a2874749fd9324852c4a73b447",
            "5d318b3468574316b4ddfbfcbcb6be74",
            "a31000fd84eb45e5a5c00dd49ce3e611",
            "8564ce260d124dc1b520ccb28a790d91",
            "5c598d83784e4e818940107d1ab5581a",
            "af1d50b7b1ae4130ab928c64bcb23496",
            "2cb3ddf5fe684a7cbbf88b9c5b4da33d",
            "40b1cbeee1ae467d9c1c62ed04622ca0",
            "afa8093dd637461aa278116d8fc08b54",
            "dbe04a66d77d4168b295a20c87081fb5",
            "af6d5ac5316049d2bb3a98d13dfc1b45",
            "919fd43389eb49a9b30f0036b11ccda9",
            "896cc517e3e54c35b08f8454f51b0fb9",
            "4f26104b947a45a4aa7ff4f55bffcd6a"
          ]
        },
        "id": "mB_MBq-J1fvJ",
        "outputId": "a6082144-1237-41d1-a746-cfc09518d14f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration google--MusicCaps-7925612b943f961b\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/google___csv/google--MusicCaps-7925612b943f961b/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Limiting to 2 examples\n",
            "    "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "#0:   0%|          | 0/1 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a63677c481054b548210b6cd931d599e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "#1:   0%|          | 0/1 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8564ce260d124dc1b520ccb28a790d91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['ytid', 'start_s', 'end_s', 'audioset_positive_labels', 'aspect_list', 'caption', 'author_id', 'is_balanced_subset', 'is_audioset_eval', 'audio', 'download_status'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Hubert"
      ],
      "metadata": {
        "id": "m-4e_Cd81EdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a script that downloads the following files: https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt, https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960_L9_km500.bin\n",
        "# into a new folder in the current directory called hubert.\n",
        "\n",
        "def download_file(url, file_name):\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        response = requests.get(url)\n",
        "        file.write(response.content)\n",
        "\n",
        "def get_hubert():\n",
        "    # Create a folder called hubert\n",
        "    os.mkdir(\"hubert\")\n",
        "\n",
        "    # Download the files\n",
        "    download_file(\"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\", \"hubert/hubert_base_ls960.pt\")\n",
        "    download_file(\"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960_L9_km500.bin\", \"hubert/hubert_base_ls960_L9_km500.bin\")\n"
      ],
      "metadata": {
        "id": "VqZ7DudV8OYo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_hubert()"
      ],
      "metadata": {
        "id": "s_tv61QI8VLS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Transformer Training"
      ],
      "metadata": {
        "id": "B_ANyfo11J3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
        "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
        ").cuda()\n",
        "\n",
        "trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
        "    folder ='./music_data',\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 1\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOmxhZ6Au5ZA",
        "outputId": "df8e188c-bbea-493b-b422-c5186d497028"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with dataset of 1 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: loss: 6.232154369354248\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: valid loss 5.5342841148376465\n",
            "0: saving model to results\n",
            "training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Soundstream Training"
      ],
      "metadata": {
        "id": "V_2qdlTD1QqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "trainer = SoundStreamTrainer(\n",
        "    soundstream,\n",
        "    folder = './music_data',\n",
        "    batch_size = 4,\n",
        "    grad_accum_every = 8,         # effective batch size of 32\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 2,\n",
        "    save_model_every = 4,\n",
        "    num_train_steps = 9\n",
        ").cuda()\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK7EN7OAsz97",
        "outputId": "81ea7432-2684-4689-b565-874f6773f94e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with dataset of 1 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) y\n",
            "0: soundstream total loss: 169.771, soundstream recon loss: 0.730 | discr (scale 1) loss: 2.001 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
            "0: saving to results\n",
            "0: saving model to results\n",
            "1: soundstream total loss: 153.024, soundstream recon loss: 0.388 | discr (scale 1) loss: 1.972 | discr (scale 0.5) loss: 1.967 | discr (scale 0.25) loss: 1.976\n",
            "2: soundstream total loss: 149.553, soundstream recon loss: 0.364 | discr (scale 1) loss: 1.959 | discr (scale 0.5) loss: 1.934 | discr (scale 0.25) loss: 1.952\n",
            "2: saving to results\n",
            "3: soundstream total loss: 150.173, soundstream recon loss: 0.386 | discr (scale 1) loss: 1.959 | discr (scale 0.5) loss: 1.923 | discr (scale 0.25) loss: 1.923\n",
            "4: soundstream total loss: 117.401, soundstream recon loss: 0.156 | discr (scale 1) loss: 1.968 | discr (scale 0.5) loss: 1.961 | discr (scale 0.25) loss: 1.955\n",
            "4: saving to results\n",
            "4: saving model to results\n",
            "5: soundstream total loss: 104.703, soundstream recon loss: 0.083 | discr (scale 1) loss: 1.937 | discr (scale 0.5) loss: 1.956 | discr (scale 0.25) loss: 1.924\n",
            "6: soundstream total loss: 112.237, soundstream recon loss: 0.139 | discr (scale 1) loss: 1.922 | discr (scale 0.5) loss: 1.914 | discr (scale 0.25) loss: 1.921\n",
            "6: saving to results\n",
            "7: soundstream total loss: 106.458, soundstream recon loss: 0.107 | discr (scale 1) loss: 1.944 | discr (scale 0.5) loss: 1.838 | discr (scale 0.25) loss: 1.909\n",
            "8: soundstream total loss: 97.325, soundstream recon loss: 0.067 | discr (scale 1) loss: 1.924 | discr (scale 0.5) loss: 1.768 | discr (scale 0.25) loss: 1.848\n",
            "8: saving to results\n",
            "8: saving model to results\n",
            "training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coarse Transformer Training"
      ],
      "metadata": {
        "id": "vcfNWiRq1Xue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
        "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
        ")\n",
        "\n",
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(\"./results/soundstream.8.pt\")\n",
        "\n",
        "coarse_transformer = CoarseTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    codebook_size = 1024,\n",
        "    num_coarse_quantizers = 3,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True\n",
        ")\n",
        "\n",
        "trainer = CoarseTransformerTrainer(\n",
        "    transformer = coarse_transformer,\n",
        "    soundstream = soundstream,\n",
        "    audio_conditioner = quantizer,\n",
        "    wav2vec = wav2vec,\n",
        "    folder = './music_data',\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 2,\n",
        "    save_model_every = 4,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w1ZJ_ipr9qV",
        "outputId": "d7cf5b67-614a-47ac-f99c-7ab76784b1bc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with dataset of 1 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: loss: 56.20853805541992\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: valid loss 29.35236930847168\n",
            "0: saving model to results\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "1: loss: 25.14291763305664\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "2: loss: 21.656782150268555\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "2: valid loss 34.56642532348633\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "3: loss: 19.115047454833984\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "4: loss: 21.112459182739258\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "4: valid loss 25.849586486816406\n",
            "4: saving model to results\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "5: loss: 20.258272171020508\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "6: loss: 19.552349090576172\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "6: valid loss 28.50279426574707\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "7: loss: 17.972370147705078\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "8: loss: 14.334244728088379\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "8: valid loss 26.385732650756836\n",
            "8: saving model to results\n",
            "training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Transformer Training"
      ],
      "metadata": {
        "id": "yZMJqr5-1dyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(\"./results/soundstream.8.pt\")\n",
        "\n",
        "fine_transformer = FineTransformer(\n",
        "    num_coarse_quantizers = 3,\n",
        "    num_fine_quantizers = 5,\n",
        "    codebook_size = 1024,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True\n",
        ")\n",
        "\n",
        "trainer = FineTransformerTrainer(\n",
        "    transformer = fine_transformer,\n",
        "    soundstream = soundstream,\n",
        "    audio_conditioner = quantizer,\n",
        "    folder = './music_data',\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbSJFGfVty4c",
        "outputId": "4082dcf7-ce45-4e96-e0e6-99be0c035217"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with dataset of 1 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: loss: 64.39331817626953\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "0: valid loss 40.131996154785156\n",
            "0: saving model to results\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "1: loss: 38.22500991821289\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "2: loss: 18.497600555419922\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "3: loss: 16.688297271728516\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "4: loss: 14.202750205993652\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "5: loss: 17.568578720092773\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "6: loss: 18.22222900390625\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "7: loss: 14.044793128967285\n",
            "spectrogram yielded shape of (65, 854), but had to be cropped to (64, 848) to be patchified for transformer\n",
            "8: loss: 10.31991958618164\n",
            "training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate"
      ],
      "metadata": {
        "id": "fspjCAyr1jjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from musiclm_pytorch import MusicLM\n",
        "\n",
        "audiolm = AudioLM(\n",
        "    wav2vec = wav2vec,\n",
        "    soundstream = soundstream,\n",
        "    semantic_transformer = semantic_transformer,\n",
        "    coarse_transformer = coarse_transformer,\n",
        "    fine_transformer = fine_transformer\n",
        ")\n",
        "musiclm = MusicLM(\n",
        "    audio_lm = audiolm,\n",
        "    mulan_embed_quantizer = quantizer\n",
        ")\n",
        "\n",
        "music = musiclm(['the crystalline sounds of the piano in a ballroom']) # torch.Tensor"
      ],
      "metadata": {
        "id": "O4tklVJsvBDT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4f74f4-2580-446f-d7be-d6f9ddb8f2e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "generating semantic:   0%|          | 3/2048 [00:00<01:09, 29.31it/s]\n",
            "generating coarse: 100%|██████████| 512/512 [01:38<00:00,  5.20it/s]\n",
            "generating fine: 100%|██████████| 512/512 [17:50<00:00,  2.09s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "output_path = \"out1.wav\"\n",
        "sample_rate = 44100\n",
        "torchaudio.save(output_path, music.cpu(), sample_rate)"
      ],
      "metadata": {
        "id": "PFbnpBUK2XkS"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}